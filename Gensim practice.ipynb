{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533fc348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe2ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e5d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989f4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1744f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5def97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f352dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "#lets associate each work in teh corpus with a unique integer ID using the gensim.corpora.Dictionary class.\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968e4b6",
   "metadata": {},
   "source": [
    "because our corpus is small there are only 12 different tokens in the dictionary. Large corpuses can have dictionaries that range in the hundres of thousands of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec56857",
   "metadata": {},
   "source": [
    "<h3>Vector</h3>\n",
    "To infer the latest structure we need to represent the documnets mathematically. We can do this by representing ech document as a vector of features.\n",
    "A single feature may be thought of as a question-answer pair:\n",
    "    \n",
    "    1. how many times does the word splonge appear in the docuiment? 0\n",
    "    2. How many paragraphs does the document consist of? 2\n",
    "    3. How many fonts does the document use? 5\n",
    "    \n",
    "The representation of the features of the document can become a series of pairs like:\n",
    "    (1, 0),(2, 2), (3, 5). This is known as a dense vector.\n",
    "    \n",
    "\n",
    "    Note: Only questions to which the answer is (or can be converted to) a single floating point number are allowed in Gensim,\n",
    "    \n",
    "    Note: In pratice vectors can contain many zero values. To save memory Gensim omits all vector elements with value 0.0.\n",
    "    \n",
    "Assuming the questions are the same, we can comare the vectors of two different documents to each other. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb46b31",
   "metadata": {},
   "source": [
    "<h3> Bag of words model</h3>\n",
    "\n",
    "Another approach to represent a document as a vector is the 'bag of words' model. Under the bag of words model each document is represented by a vector containing the frequency counts of each word in the dictionary.\n",
    "E.g. ['coffee','milk','sugar','spoon'] could be the dictionary.\n",
    "\n",
    "A document consisting of the string \"coffee milk coffee\" would be represented in a bag of words model for this dict by the vector: [2,1,0,0]\n",
    "\n",
    "One of the main properties of the 'bag of words' model is that it completely ignores the order of the tokens in the document that is encoded.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5774d1",
   "metadata": {},
   "source": [
    "Ther whole corpus can be converted into a list of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c125c6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeff5ba",
   "metadata": {},
   "source": [
    "note: the distinction between a document and a vector is that the former is text, and the latter is a mathematically convenient representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4c350",
   "metadata": {},
   "source": [
    "note: depending how representation was obtained, two different documents may have the same vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76503e39",
   "metadata": {},
   "source": [
    "The vectorised corpus can now be transformed using models. \n",
    "One simple example of a model is tf-idf. The tf-idf model transforms vectors from a bag of words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7ba56bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "#train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "#transform the \"system minors\" string\n",
    "\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84294c",
   "metadata": {},
   "source": [
    "The tfidf model returns a list of tuples where the first entry is the token ID and the second entry is the tf-idf weighting. The results above show the tf-idf weighting of 0.59 for the word 'system' (5), which appeared 11 times in the original corpus. The second tuple shows the word \"minors\" which only appears twice in the original corpus and is assigned with a tf-idf weighting of 0.80 by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a063ba3",
   "metadata": {},
   "source": [
    "Once the model has been created you can perform other transformations to it.\n",
    "\n",
    "For example: transform the whole corpus via Tfldf and index it in preparation for similarity queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f9364d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463d0a1",
   "metadata": {},
   "source": [
    "Now if we want to query the similarity of our query document against every document in the corpus:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab54ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = 'system engineering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc1ee1",
   "metadata": {},
   "source": [
    "<h2> Summary</h2>\n",
    "\n",
    "The core concepts of gensim are:\n",
    "<li>1. Document: some text\n",
    "<li>2. Corpus: a collection of documents\n",
    "<li>3. Vector: a mathematically convenient representation of a document\n",
    "<li>4. Model: an algorithm for transforming vectors from one representation to another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104a157",
   "metadata": {},
   "source": [
    "<h4>What we did in this notebook</h4>\n",
    "\n",
    "1. Collected a corpus of documents\n",
    "2. Transformed the documents to a vector space representation\n",
    "3. Created a model that transformed the original vector representation to Tfldf.\n",
    "4. Used the model to calculate the similarity between a query document and all documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f408a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
